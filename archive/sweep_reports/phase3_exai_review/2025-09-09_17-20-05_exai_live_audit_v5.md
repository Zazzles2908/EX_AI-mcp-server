## EXAI Live Audit (v5)

### Inputs (files)

- src\providers\openai_compatible.py

- src\providers\glm.py

- src\providers\kimi.py

- src\providers\custom.py

- src\providers\openrouter.py

- utils\http_client.py

- tools\glm_files.py


### Kimi audit

- Model: kimi-k2-0711-preview

- Response:


### High-Priority Issues

- **GLM upload_file() fallback to HTTP is untested**  
  The `GLMModelProvider.upload_file()` method falls back to `HttpClient.post_multipart()` if the SDK is unavailable, but the `HttpClient` class is imported from `utils.http_client` which is not shown. Verify that `post_multipart()` correctly handles GLM’s `/files` endpoint (auth header, multipart format, JSON parsing). Missing or mis-matched implementation will break file upload entirely.

- **GLM multi-file chat silently ignores file content**  
  `GLMMultiFileChatTool` uploads files but never retrieves their content; it only lists filenames in the system prompt. This contradicts the tool description (“summarized as system content”) and will mislead users. Either implement GLM’s file-retrieval API (if it exists) or update the description to clarify that only filenames are provided.

- **Kimi `upload_file()` uses wrong purpose value**  
  `KimiModelProvider.upload_file()` defaults to `purpose="file-extract"` but Moonshot’s docs require `purpose="file-extract"` only for assistants; for chat completions the correct purpose is `"file-extract"` or `"assistants"` depending on endpoint. Confirm the right value or make it configurable.

- **CustomProvider dummy key leaks into logs**  
  When no API key is supplied, `CustomProvider` sets `api_key = "dummy-key-for-unauthenticated-endpoint"` and logs the base URL. This dummy key could appear in exception traces or debug output; sanitize or mask it.

- **OpenRouter retry logic ignores 429 token errors**  
  `_is_error_retryable()` in `OpenAICompatibleProvider` marks 429 “tokens” errors as non-retryable, but OpenRouter may return 429 for per-minute limits that *are* retryable. Ensure the heuristic aligns with OpenRouter’s actual rate-limit headers.

### Medium-Priority Issues

- **GLM `count_tokens()` is crude**  
  Uses `len(text)//4` which is inaccurate for Chinese text and GLM’s tokenizer. Consider integrating GLM’s `/tokenize` endpoint or tiktoken with GLM’s encoding.

- **Kimi `_build_payload()` duplicates image handling**  
  The method re-implements base64 encoding and size checks already present in `OpenAICompatibleProvider._process_image()`. Deduplicate to avoid drift.

- **Streaming path in `OpenAICompatibleProvider` lacks back-pressure**  
  The streaming loop collects all chunks in memory (`content_parts`). For very long responses this can exhaust RAM. Consider yielding chunks instead.

- **Missing timeout override for GLM HTTP fallback**  
  `HttpClient` defaults to 60 s; GLM large file uploads may exceed this. Allow override via env var or parameter.

### Low-Priority / Follow-ups

- **Add retry decorator to GLM HTTP fallback**  
  The HTTP branch in `GLMModelProvider.generate_content()` has no retries; transient network errors will fail immediately.

- **Document GLM file size limits**  
  Tool descriptions mention “~100 MB/file” but GLM’s actual limit is undocumented; verify and expose.

- **Standardize file upload return schema**  
  GLM returns `{file_id, filename}`, Kimi returns `{id, filename, bytes}`; unify for downstream consumers.

- **Expose `purpose` parameter in GLM tools**  
  Currently hard-coded to “agent”; allow callers to specify “assistants” or other GLM-supported purposes.



### GLM audit

- Model: glm-4.5

- Uploaded files:

  - openai_compatible.py (id=1757402466368-af6ad8aa319c487fa54bb499f12fe493.py)

  - glm.py (id=1757402466698-7a2bf0844268454b9e319686126c4fdd.py)

  - kimi.py (id=1757402467045-6b2dd4512c1e4574a1231fe8b50ba3ce.py)

  - custom.py (id=1757402467374-101addfa40cf4f0395cc70a5e79389e7.py)

  - openrouter.py (id=1757402467685-e1b80d9d4c8c4889a41e0288d87e273d.py)

  - http_client.py (id=1757402467978-d2f2059c0b58416a9ef9aafc102d2299.py)

  - glm_files.py (id=1757402468280-081ce48f80584506acfd4b8b53211aa1.py)

- Response:


# Audit Findings for LLM Provider Files

## High Priority Issues
- **Inconsistent error handling across providers**: Different providers (Kimi, GLM) likely have unique error formats, risking unhandled exceptions. Implement a unified error normalization layer.
- **Missing or inadequate retry mechanisms**: Transient network issues or rate limits could cause failures without exponential backoff. Add configurable retry logic with jitter.
- **Streaming response inconsistencies**: Each provider's streaming implementation may differ, potentially causing incomplete responses or memory issues. Standardize streaming handling with proper buffering.

## Medium Priority Issues
- **API key security**: Keys might be stored in code or config without proper encryption. Ensure keys are loaded from secure sources (environment variables, secret managers).
- **Parameter validation gaps**: Invalid parameters could be passed to provider APIs, causing failures. Add strict validation before API calls.
- **Timeout handling**: Requests may hang without proper timeout configuration. Implement reasonable default timeouts with configurability.

## Lower Priority Issues
- **OpenAI compatibility surface deviations**: Minor differences in response format or parameter names could break client expectations. Audit against OpenAI API spec.
- **Memory management for large responses**: Potential memory leaks when handling large responses, especially in streaming mode. Implement proper resource cleanup.
- **Logging and monitoring**: Insufficient logging makes debugging difficult. Add structured logging with appropriate levels for requests, responses, and errors.

## Recommendations
1. Create a base provider class with common functionality (retries, error handling, timeouts)
2. Implement comprehensive integration tests for each provider
3. Add provider-specific documentation for any API deviations
4. Consider using a dependency injection pattern for easier testing and configuration
5. Implement circuit breakers to prevent cascading failures during provider outages

