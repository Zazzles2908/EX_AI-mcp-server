# --- Guardrails (authoritative minimal defaults) ---
# Keep this block short: one line per requirement. See .env.example for details.
DEFAULT_MODEL=glm-4.5-flash          # fast manager default to avoid 'auto' rejections in some tools; router still available when explicitly requested
ROUTER_ENABLED=true                  # enable cost-aware routing
EX_ROUTING_PROFILE=balanced          # speed|balanced|quality
UI_PROFILE=compact                   # keep tool menu small
SLIM_SCHEMAS=false                    # lighter schemas for heavy tools
EXPERT_ANALYSIS_MODE=enabled         # enable expert passes by default
DEFAULT_USE_ASSISTANT_MODEL=true     # allow assistant/expert model by default
ENABLE_CONSENSUS_AUTOMODE=false      # require explicit model list for consensus
MIN_CONSENSUS_MODELS=1               # if automode is enabled later
MAX_CONSENSUS_MODELS=2               # cap fan-out to control cost
SECURE_INPUTS_ENFORCED=false          # size/extension checks for file inputs
CHUNKED_READER_ENABLED=false         # default off for unit tests; enable before server restart to activate chunked embeddings
POLICY_EXACT_TOOLSET=false            # stable toolset in fallback paths
LOG_LEVEL=INFO                       # structured info logs
LOG_FORMAT=plain                     # structured logs
ACTIVITY_LOG=true                    # enable activity log
EX_TOOLCALL_LOG_PATH=.logs/toolcalls.jsonl  # JSONL tool-call log (ensure path exists)
GLM_FLASH_MODEL=glm-4.5-flash        # router hint: flash
GLM_QUALITY_MODEL=glm-4.5            # router hint: quality
KIMI_DEFAULT_MODEL=kimi-k2-0905-preview # router hint for Kimi
EX_HOTRELOAD_ENV=true                # live-toggle flags safely
EX_TOOL_HUB_ENABLED=true            # enable ToolHub adapter (non-breaking wrapper over ToolRegistry)
WORKFLOWS_PREFER_KIMI=true           # prefer Kimi/Moonshot for long-context; GLM remains manager for simple tasks
RESILIENT_RETRIES=2                  # provider call retries (adaptive backoff)
RESILIENT_BACKOFF_SECS=1.8           # initial backoff (seconds); increases with jitter and caps
KIMI_SPEED_MODEL=kimi-k2-0711-preview # avoid turbo unless explicitly requested
KIMI_FALLBACK_ORDER=kimi-k2-0905-preview,kimi-k2-0711-preview # exclude turbo/thinking by default
KIMI_ALLOW_EXPENSIVE_THINKING_FALLBACK=false # do not fall back to kimi-thinking unless explicitly enabled
ROUTER_DIAGNOSTICS_ENABLED=true      # enable structured route_diagnostics logging
ROUTER_LOG_LEVEL=INFO                # log level for router
EXAI_WS_COMPAT_TEXT=true              # include a top-level 'text' field concatenating output texts

# Router preflight should not make background paid calls
ROUTER_PREFLIGHT_CHAT=false
# Explicitly allow native providers only
ALLOWED_PROVIDERS=KIMI,GLM

# --- End guardrails ---

# Moonshot/Kimi
KIMI_API_KEY=

KIMI_API_URL=https://api.moonshot.ai/v1

# ZhipuAI GLM

GLM_API_KEY=

GLM_API_URL=https://open.bigmodel.cn/api/paas/v4

# Optional providers
# OpenRouter (optional; not enabled by default in this deployment)
OPENROUTER_API_KEY=
OPENROUTER_ALLOWED_MODELS=
OPENROUTER_REFERER=
OPENROUTER_TITLE=EX-AI-MCP-Server
# Note: Optional OpenRouter tests remain disabled by default. Set OPENROUTER_TESTS_ENABLED=true to run them.

# Custom OpenAI-compatible endpoint (optional)
CUSTOM_API_URL=
CUSTOM_API_KEY=

# Auggie integration
# AUGGIE_CONFIG=C:\\Project\\EX-AI-MCP-Server\\auggie-config.json

# =========================
# Server Configuration
# =========================
# Unique server process ID (used by MCP clients to identify this server instance)
MCP_SERVER_ID=ex-server
# Display name presented to MCP clients during initialization
MCP_SERVER_NAME=exai
# Optional: load env from a different file path
# ENV_FILE=

# Daemon (WebSocket)
EXAI_WS_HOST=127.0.0.1
EXAI_WS_PORT=8765
EXAI_WS_TOKEN=
EXAI_WS_MAX_BYTES=33554432
EXAI_WS_OPEN_TIMEOUT=60

EXAI_WS_PING_INTERVAL=45
EXAI_WS_PING_TIMEOUT=120
EXAI_WS_CALL_TIMEOUT=180  # Raised to 180s to align with daemon/shim and prevent premature client aborts
EXAI_WS_SESSION_MAX_INFLIGHT=12
EXAI_WS_GLOBAL_MAX_INFLIGHT=48
EXAI_WS_KIMI_MAX_INFLIGHT=10
EXAI_WS_GLM_MAX_INFLIGHT=10

EXAI_WS_PROGRESS_INTERVAL_SECS=2.0

EXAI_WS_INFLIGHT_TTL_SECS=180
EXAI_WS_RETRY_AFTER_SECS=1

# Expert phase client-abort mitigation (server-side fixes)
EXAI_WS_EXPERT_KEEPALIVE_MS=1500
EXAI_WS_EXPERT_SOFT_DEADLINE_SECS=60   # ensure non-empty expert block within 60s to prevent client idle timeouts
EXAI_WS_EXPERT_MICROSTEP=true

# Analyze-specific expert overrides (validated via ThinkDeep)
ANALYZE_EXPERT_TIMEOUT_SECS=60
ANALYZE_HEARTBEAT_INTERVAL_SECS=7

EXAI_WS_DISABLE_COALESCE_FOR_TOOLS=kimi_chat_with_tools,kimi_multi_file_chat,analyze,codereview,testgen,debug,thinkdeep,tracer,planner,refactor,secaudit
# Logging configuration
# LOG_LEVEL: DEBUG | INFO | WARNING | ERROR | CRITICAL
# # LOG_LEVEL=DEBUG  # superseded by top Guardrails  # superseded by top Guardrails
# # LOG_FORMAT=plain  # superseded by top Guardrails  # superseded by top Guardrails

# Defaults and feature flags
# DEFAULT_MODEL: Default model used when a tool does not specify (keep your preferred default)
# Examples: auto | glm-4.5-flash | kimi-moonshot-v1-8k | openrouter/<model>

# LOCALE: Force response language/locale (e.g., en-US, fr-FR). Leave empty for English.
LOCALE=
# Prefer free-tier models when available (if provider supports it)
PREFER_FREE_TIER=false
# Enable startup config validation (recommended)
# Dispatcher modularization (env-gated; default off)
EX_USE_DISPATCHER=false
# AI Manager (env-gated; default off)
EX_AI_MANAGER_ENABLED=true
EX_AI_MANAGER_STRATEGY=manager-first
EX_AI_MANAGER_LOG_LEVEL=INFO
EX_AI_MANAGER_ADVISORY=true

EX_AI_MANAGER_ROUTE=false
ENABLE_SMART_CHAT=false
ENABLE_ZHIPU_NATIVE=false
ENABLE_MOONSHOT_NATIVE=false



ENABLE_CONFIG_VALIDATOR=true
# Experimental: enable metadata-informed model selection
# Allow external absolute paths (opt-in). Use with explicit prefixes only.
EX_ALLOW_EXTERNAL_PATHS=true
# Comma-separated absolute prefixes that are permitted for read-only access by tools
EX_ALLOWED_EXTERNAL_PREFIXES=C:/Project/Personal_AI_Agent

ENABLE_METADATA_SELECTION=true
# Disable specific tools (comma-separated). Example: DISABLED_TOOLS=debug,precommit
DISABLED_TOOLS=
# Limit hub to 12 core tools (lean mode)
LEAN_MODE=false
STRICT_LEAN=false
LEAN_TOOLS=status,chat,analyze,codereview,debug,planner,refactor,testgen,thinkdeep,tracer,precommit,secaudit,stream_demo


# Tools registry gating (off by default; enable only at end of batch)
TOOLS_CORE_ONLY=true              # core-only gating enabled for validation
# optional comma list to include beyond core (left empty)
TOOLS_ALLOWLIST=chat,kimi_upload_and_extract,kimi_multi_file_chat,kimi_chat_with_tools,glm_upload_file,glm_multi_file_chat,glm_agent_chat,glm_agent_get_result,glm_agent_conversation

# OpenAI-compatible alias for Kimi (for clients using OpenAI-style base_url)
# OPENAI_BASE_URL=https://api.moonshot.ai/v1

# Maximum tokens the MCP server should emit per response (safe default)
# Thinking tool defaults
DEFAULT_THINKING_MODE_THINKDEEP=high
THINK_ROUTING_ENABLED=true
# Agentic ThinkDeep routing policy
THINKDEEP_FAST_EXPERT=true
AGENTIC_THINKDEEP_POLICY=quality
THINKDEEP_OVERRIDE_EXPLICIT=true       # honor fast-expert even when explicit models are passed for stability


# Cache (in-memory by default)
CACHE_BACKEND=memory
# Enable provider file-id cache (safe client-side reuse)
FILECACHE_ENABLED=true

CACHE_TTL_SEC=10800
CACHE_MAX_ITEMS=1000
# REDIS_URL=redis://localhost:6379/0  # reserved for future

# Timezone (optional)
# TZ=UTC
EXPERT_ANALYSIS_TIMEOUT_SECS=90

WORKFLOW_STEP_TIMEOUT_SECS=120


ANALYZE_ALLOW_DEFAULT_ROOT=true
EXAI_SHIM_RPC_TIMEOUT=150
EXAI_SHIM_ACK_GRACE_SECS=120


# =========================
# Remote Server (Network Mode, optional)
# =========================
# Bind host/port
MCP_REMOTE_HOST=0.0.0.0
MCP_REMOTE_PORT=7800
# Base path for MCP SSE/WebSocket APIs
MCP_BASE_PATH=/mcp
# Alternative/common base paths (comma-separated)
MCP_ALT_PATHS=/sse,/v1/sse
# Bearer token required for remote access (set a strong secret in production)
MCP_AUTH_TOKEN=
# CORS origins (comma-separated) or *
CORS_ORIGINS=*
STREAM_PROGRESS=true

# =========================
# Security & Performance
# =========================
# Global tool execution timeout (seconds). Tools may also apply their own internal limits.
TOOL_EXEC_TIMEOUT_SEC=90
# Soft memory cap hint for heavy tools (MB). Not enforced by Python, but tools can read this and adapt.
MAX_MEMORY_MB=1024
# Additional file/directory exclusions for scanners (comma-separated). Core exclusions are built-in.
# Example: FILE_SCAN_EXCLUDE_DIRS=.venv,venv,node_modules,build,dist,logs
FILE_SCAN_EXCLUDE_DIRS=


# =========================
# Auggie Integration (optional)
# =========================
# Path to auggie-config.json
# AUGGIE_CONFIG=C:\\Project\\EX-AI-MCP-Server\\auggie-config.json
# Poll interval for config hot-reload (seconds)
AUGGIE_CONFIG_POLL_INTERVAL=3.0


# =========================
# Provider-native Web Search & EX Unified Controls
# =========================
KIMI_ENABLE_INTERNET_SEARCH=true
GLM_ENABLE_WEB_BROWSING=true
KIMI_WEBSEARCH_SCHEMA=function

ENABLE_SMART_WEBSEARCH=true

# Unified defaults
EX_WEBSEARCH_ENABLED=true
EX_WEBSEARCH_DEFAULT_ON=true
EX_WEBSEARCH_MAX_RESULTS=5
EX_WEBSEARCH_LOCALE=en-US
EX_WEBSEARCH_SAFETY_LEVEL=standard
EX_WEBSEARCH_QUERY_TIMEOUT_MS=8000
EX_WEBSEARCH_TOTAL_TIMEOUT_MS=15000
EX_WEBSEARCH_CACHE_TTL_S=300
EX_WEBSEARCH_ALLOWED_DOMAINS=
EX_WEBSEARCH_BLOCKED_DOMAINS=

# =========================
# Tool-call Visibility & Logging
# =========================
EX_TOOLCALL_LOG_LEVEL=info
# Set a path to enable JSONL logging of sanitized tool-call events
# Ensure the directory exists and is writable by the server process
EX_TOOLCALL_LOG_PATH=.logs/toolcalls.jsonl
EX_ACTIVITY_TAIL_LAST=true
EX_ACTIVITY_MARKDOWN_DETAILS=true
EX_TOOLCALL_REDACTION=true
DISABLE_TOOL_ANNOTATIONS=true      # reduce handshake overhead under parallel calls


# For consistent dropdowns even without web_search:
EX_ALWAYS_TOOLCALL_METADATA=true
# To confirm Activity tool tail points to the right log:
EX_ACTIVITY_LOG_PATH=C:/Project/EX-AI-MCP-Server/logs/mcp_server.log
# For web-search events in JSONL:
EX_WEBSEARCH_DEFAULT_ON=true
# Enable per-call env hot reload and top summary for visibility
EX_HOTRELOAD_ENV=true
EX_ACTIVITY_SUMMARY_AT_TOP=false
EX_ACTIVITY_FORCE_FIRST=false

EX_ENSURE_NONEMPTY_FIRST=true       # ensure first block is never empty for first-block-only clients


# Stability and watchdog controls (Phase D)
EX_HTTP_TIMEOUT_SECONDS=60
EX_TOOL_TIMEOUT_SECONDS=300
FALLBACK_ATTEMPT_TIMEOUT_SECS=60  # per-attempt dispatcher timeout for file_chat fallback
EX_HEARTBEAT_SECONDS=8
EX_WATCHDOG_WARN_SECONDS=45
EX_WATCHDOG_ERROR_SECONDS=120
EX_MIRROR_ACTIVITY_TO_JSONL=true


# Claude-aware tool filtering and defaults
CLAUDE_TOOL_ALLOWLIST=status,chat,analyze,codereview,debug,planner,refactor,testgen,thinkdeep,tracer,precommit,secaudit,stream_demo
CLAUDE_TOOL_DENYLIST=
CLAUDE_DEFAULTS_USE_WEBSEARCH=true
CLAUDE_DEFAULT_THINKING_MODE=medium
CLAUDE_MAX_WORKFLOW_STEPS=3

THINKDEEP_USE_ASSISTANT_MODEL_DEFAULT=true

# Prefer long context models when available
THINKDEEP_EXPERT_TIMEOUT_SECS=60
THINKDEEP_HEARTBEAT_INTERVAL_SECS=0.5
KIMI_THINKING_MODEL=kimi-k2-0905-preview
EX_PREFER_LONG_CONTEXT=true

# Allow relative paths (resolved under project root) for friendlier Claude usage
EX_ALLOW_RELATIVE_PATHS=true


# =========================
# Workflow Auto-Continue (EX-AI)
# =========================
# Enable automatic progression across workflow steps when a tool returns pause_for_*
EX_AUTOCONTINUE_WORKFLOWS=true
# Restrict auto-continue to thinkdeep only (recommended)
EX_AUTOCONTINUE_ONLY_THINKDEEP=true
# Upper bound of consecutive auto-continued steps per server call
EX_AUTOCONTINUE_MAX_STEPS=3

# =========================
# Allowed Models (Explicit Whitelist)
# =========================
# When set, only the models listed here (including aliases) are exposed/usable for each provider.
# Comment these out to allow all models from a provider.
# Canonical allowlists aligned with providers/kimi.py and providers/glm.py
KIMI_ALLOWED_MODELS=kimi-k2-0905-preview,kimi-k2-0711-preview,kimi-k2-turbo-preview,moonshot-v1-8k,moonshot-v1-32k,moonshot-v1-128k,moonshot-v1-8k-vision-preview,moonshot-v1-32k-vision-preview,moonshot-v1-128k-vision-preview,kimi-latest,kimi-thinking-preview
GLM_ALLOWED_MODELS=glm-4.5-flash,glm-4.5,glm-4.5-air

# GLM Deep Thinking mode default (enabled|disabled)
GLM_THINKING_MODE=enabled


# GLM Agent API base for agent endpoints
# GLM_AGENT_API_URL is deprecated in favor of GLM_API_URL above
# GLM_AGENT_API_URL=

# Kimi Tool-Use defaults
KIMI_ENABLE_INTERNET_TOOL=true
KIMI_INTERNET_TOOL_SPEC={"type":"function","function":{"name":"web_search","parameters":{"type":"object","properties":{"query":{"type":"string"}},"required":["query"]}}}
KIMI_DEFAULT_TOOL_CHOICE=auto

# Timeout precedence (P0 validation):
# 1) Tool execute cap (KIMI_MF_CHAT_TIMEOUT_SECS) <= 50s
# 2) Orchestrator per-attempt cap (FALLBACK_ATTEMPT_TIMEOUT_SECS) = 60s
# 3) Client WS call timeout (EXAI_WS_CALL_TIMEOUT) = 90s (margin)
KIMI_CHAT_TOOL_TIMEOUT_SECS=90
# Kimi files/extract advanced controls and diagnostic mode
KIMI_FILES_FETCH_RETRIES=3
KIMI_FILES_FETCH_BACKOFF=0.8
KIMI_FILES_FETCH_INITIAL_DELAY=0.5
KIMI_MF_INJECT_MAX_BYTES=51200
# Optional MoonPalace diagnostic gateway (off by default)
KIMI_DIAG_MOONPALACE=false
KIMI_MOONPALACE_BASE_URL=

KIMI_MF_CHAT_TIMEOUT_SECS=50
KIMI_CHAT_TOOL_TIMEOUT_WEB_SECS=300
KIMI_DEFAULT_READ_TIMEOUT_SECS=120
KIMI_FILES_FETCH_TIMEOUT_SECS=25
GLM_MF_CHAT_TIMEOUT_SECS=60

# Advisory client-side max upload size checks (MB); provider imposes real limits server-side
KIMI_FILES_MAX_SIZE_MB=20
GLM_FILES_MAX_SIZE_MB=50

# Upload timeouts for large files (seconds)
GLM_FILE_UPLOAD_TIMEOUT_SECS=180
FILE_UPLOAD_TIMEOUT_SECS=180
# Web search backend for Kimi function tool loop: duckduckgo | tavily | bing
SEARCH_BACKEND=duckduckgo
TAVILY_API_KEY=
BING_SEARCH_API_KEY=

# Final-step embedding guardrails
EXPERT_INCLUDE_HISTORY=true  # enable convo history files for expert analysis (unit tests expect this)
EXPERT_MAX_FILES=10
EXPERT_TIME_BUDGET_SECS=90
EXPERT_FALLBACK_AFTER_SECS=12
EXPERT_FALLBACK_ENABLED=false
EXPERT_HEARTBEAT_INTERVAL_SECS=5

# Environment for local GH tooling integration
# IMPORTANT:
# - Do NOT commit secrets here. This file is ignored by git, but avoid storing tokens in plaintext.
# - Prefer dynamic auth: run scripts/set-gh-token.ps1 to export GH_TOKEN from `gh auth token`.

# Default host for GitHub API operations
GH_HOST=github.com

# Prefer keyring auth via `gh auth login`; do not set GH_TOKEN here to avoid overriding the credential helper.

# Optional: path to gh.exe for the MCP server if gh is not on PATH
# AUGGIE_GH_PATH=C:\Program Files\GitHub CLI\gh.exe

# Default repo path used by gh-mcp tools when no path is passed by the client
GH_MCP_DEFAULT_REPO_PATH=C:/Project/EX-AI-MCP-Server
# Named repo paths (optional, for convenience or future tools)
GH_MCP_REPO_EXAI=C:/Project/EX-AI-MCP-Server
GH_MCP_REPO_PERSONAL_AI_AGENT=C:/Project/Personal_AI_Agent
GH_MCP_REPO_GH_CLI=C:/Project/Git_cli

GH_SSH_KEY=ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIBfr9oo0OKS3KRlS3RakLB9mk8CeuU8+6qbaglrCg5Sj
GH_SSH_EMAIL=jajireen1@gmail.com

# GitHub: prefer `gh auth login` (uses OS keyring). Leave GH_TOKEN empty unless running headless.
GH_TOKEN=


# ===== Phase F/G productionization defaults =====
# Duplicate provider keys for new managers (kept local; .env is not committed)

# Paths for new sinks (file-based metrics/audit)
METRICS_LOG_PATH=logs/metrics.jsonl
AUDIT_LOG_PATH=logs/mcp_audit.log

# Optional RBAC policy sources (prefer file; inline JSON as fallback)
# RBAC_POLICY_FILE=docs/security/rbac_policy.json
# RBAC_POLICY_JSON={"admin":["*"],"developer":["analyze","codereview","testgen","thinkdeep","planner"],"viewer":["thinkdeep","planner"]}

HEALTH_MONITOR_ENABLED=true
# =========================
# Supabase MCP Bridge (Pro‑ready)
# =========================
# Enable the Supabase‑native MCP bridge. When true, clients can call the gateway Edge Function
# instead of the Python server for tool workflows. This remains backward‑compatible.
USE_SUPABASE_MCP_BRIDGE=true

# Your Supabase project reference (safe to commit; not a secret)
SUPABASE_PROJECT_REF=mxaazuhlqewmkweewyaz
# Canonical REST and Functions domains (derived from the ref)
SUPABASE_REST_URL=https://${SUPABASE_PROJECT_REF}.supabase.co
SUPABASE_FUNCTIONS_BASE_URL=https://${SUPABASE_PROJECT_REF}.functions.supabase.co

# Gateway Edge Function SSE endpoint (used by clients or by the Python server as a proxy)
MCP_GATEWAY_URL=${SUPABASE_FUNCTIONS_BASE_URL}/functions/v1/gateway
# Default header names expected by the gateway for per‑user isolation (do NOT put secrets here)
MCP_GATEWAY_HEADER_ALIAS=X-Alias
MCP_GATEWAY_HEADER_KIMI=X-Kimi-Key
MCP_GATEWAY_HEADER_GLM=X-Glm-Key

# Memory cadence (prefer Supabase Scheduled Functions on Pro; fall back to pg_cron in dev if needed)
SCHEDULED_FUNCTIONS_ENABLED=true
MEMORY_SCHEDULE_CRON=0 3 * * *
PREFER_EDGE_SCHEDULE_OVER_PG_CRON=true

# App‑level guardrails (no DB storage of provider keys; require strict RLS in code paths)
DO_NOT_STORE_PROVIDER_KEYS_IN_DB=true
RLS_STRICT=true

# Secrets source policy: in production, set SERVICE_ROLE and JWT secret via Supabase Secrets Manager
# (e.g., `supabase secrets set SUPABASE_SERVICE_ROLE_KEY=... SUPABASE_JWT_SECRET=...`)
SUPABASE_SECRETS_SOURCE=secrets_manager

# Optional Pro network hardening toggles (documentation hints; no behavior unless your code reads them)
SUPABASE_NETWORK_RESTRICTIONS=allowlist
SUPABASE_VPC_EGRESS_ENABLED=false



# SUPABASE CREDENTIALS

SUPABASE_DC=
NEXT_PUBLIC_SUPABASE_URL=
NEXT_PUBLIC_SUPABASE_ANON_KEY=
SUPABASE_PUBLISHABLE_KEY=
SUPABASE_SECRET_KEY=
SUPABASE_STANDBY_KEY=
SUPABASE_CURRENT_KEY=